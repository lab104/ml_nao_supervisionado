{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28a7a9a-84f3-47fa-9069-d37d9e4f71f6",
   "metadata": {},
   "source": [
    "# Instituto de Educação Superior de Brası́lia – IESB\n",
    "## Pós-Graduação em Inteligência Artificial\n",
    "### Disciplina de Computação Cognitiva 3 / Turma 2021-1\n",
    "#### Trabalho Final - Análise de Sentimento\n",
    "\n",
    "                          EQUIPE:\n",
    "                          - LUCAS DE SOUSA BRITO, MAT:2186330019, TURMA: 2021-1\n",
    "                          - PABLO NOGUEIRA OLIVEIRA, MAT:2186330027, TURMA: 2021-1\n",
    "                          - MATHEUS BARBOSA OLIVEIRA, MAT:2186330037, TURMA: 2021-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa4b3d",
   "metadata": {
    "id": "dbaa4b3d",
    "papermill": {
     "duration": 0.013983,
     "end_time": "2022-05-23T21:42:19.432623",
     "exception": false,
     "start_time": "2022-05-23T21:42:19.418640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Análise de sentimento da base do twitter Sentiment140\n",
    "\n",
    "### Dados de Origem\n",
    "\n",
    "* http://help.sentiment140.com/for-students\n",
    "\n",
    "| sentiment  | id | date | query_string | user | text\n",
    "| ---        | -- | -    | -            | -    | ---\n",
    "| 0=negativo | -  | -    | -            | -    | the original twitter message\n",
    "| 2=neutro   | -  | -    | -            | -    | \n",
    "| 4=positivo | -  | -    | -            | -    |\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f982a01",
   "metadata": {
    "_kg_hide-input": true,
    "id": "2f982a01",
    "papermill": {
     "duration": 0.022259,
     "end_time": "2022-05-23T21:42:19.467763",
     "exception": false,
     "start_time": "2022-05-23T21:42:19.445504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf7bd3a",
   "metadata": {
    "_kg_hide-input": true,
    "id": "0bf7bd3a",
    "papermill": {
     "duration": 4.684439,
     "end_time": "2022-05-23T21:42:24.164782",
     "exception": false,
     "start_time": "2022-05-23T21:42:19.480343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(129783)\n",
    "np.random.seed(3213)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be955320",
   "metadata": {
    "id": "be955320",
    "papermill": {
     "duration": 0.012492,
     "end_time": "2022-05-23T21:42:24.190494",
     "exception": false,
     "start_time": "2022-05-23T21:42:24.178002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importação\n",
    "\n",
    "Dados de origem: \n",
    "* http://help.sentiment140.com/for-students\n",
    "* colunas:\n",
    "   * sentiment (0=negativo, 2=neutro, 4=positivo)\n",
    "   * id\n",
    "   * date\n",
    "   * query_string\n",
    "   * user\n",
    "   * text\n",
    "\n",
    "Para este exercício:\n",
    "* apenas as colunas sentiment e text serão mantidas\n",
    "* sentimentos neutros serão descartados\n",
    "* sentimentos serão padronizados como 0=negativo e 1=positivo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ULaLv8Z3XV7G",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULaLv8Z3XV7G",
    "outputId": "896b5f22-763f-44bc-ea00-140cd968e56c"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YwNNMYW6YPH0",
   "metadata": {
    "id": "YwNNMYW6YPH0"
   },
   "outputs": [],
   "source": [
    "filename = '/content/drive/MyDrive/Colab Notebooks/Datasets/Tweets/training.1600000.processed.noemoticon.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d3478a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "55d3478a",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "167651c0-d770-44ad-c26e-e3ac0c3e822b",
    "papermill": {
     "duration": 7.151467,
     "end_time": "2022-05-23T21:42:31.354779",
     "exception": false,
     "start_time": "2022-05-23T21:42:24.203312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bloco():\n",
    "    \n",
    "    global df_original\n",
    "    \n",
    "    df_cols = ['sentiment','id','date','query_string','user','text']\n",
    "\n",
    "    df_original = pd.read_csv(\n",
    "        #\"../input/sentiment140/training.1600000.processed.noemoticon.csv\",\n",
    "        filename,\n",
    "        header=None, \n",
    "        names=df_cols,\n",
    "        encoding = \"ISO-8859-1\"\n",
    "    )\n",
    "\n",
    "    df_original.drop(\n",
    "        ['id','date','query_string','user'],\n",
    "        axis=1,\n",
    "        inplace=True\n",
    "    )\n",
    "    df_original = df_original[ df_original['sentiment'] != 2 ] \n",
    "    df_original['sentiment'] = df_original['sentiment'].apply( lambda x: 1 if x==0 else 0 )\n",
    "    return df_original\n",
    "\n",
    "bloco()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e0de3",
   "metadata": {
    "id": "652e0de3",
    "papermill": {
     "duration": 0.012591,
     "end_time": "2022-05-23T21:42:31.380663",
     "exception": false,
     "start_time": "2022-05-23T21:42:31.368072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Padronização (1)\n",
    "\n",
    "* Parte 1\n",
    "  * remove todas as tags\n",
    "  * remove urls\n",
    "  * remove identificadores de usuários \n",
    "  * remove caracteres unicode inválidos\n",
    "  * remove carcteres não textuais\n",
    "  * transforma tudo para minúsculas\n",
    "* Parte 2\n",
    "  * tokeniza usando o keras\n",
    "* Parte 3\n",
    "  * separa base de treinamento e de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcaf66e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "8dcaf66e",
    "outputId": "1c8a9d9c-1d53-4309-81a3-06a31408b0b0",
    "papermill": {
     "duration": 82.31875,
     "end_time": "2022-05-23T21:43:53.712228",
     "exception": false,
     "start_time": "2022-05-23T21:42:31.393478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data_limit = 200000\n",
    "max_words = 100000\n",
    "max_len = 200\n",
    "\n",
    "def bloco():\n",
    "    \n",
    "    global df_original\n",
    "    global df_train\n",
    "    global df_test   \n",
    "   \n",
    "    # PARTE 1 - Limpa o texto \n",
    "    import re\n",
    "    pat1 = r'@[A-Za-z0-9]+'\n",
    "    pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "    pat3 = r'<.*?>'\n",
    "    pat4 = r'&.*?;'\n",
    "    pat = r'|'.join((pat1, pat2, pat3, pat4))\n",
    "    def tweet_cleaner(text):       \n",
    "        text = re.sub(pat,'',text)\n",
    "        try:\n",
    "            text = text.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "        except:\n",
    "            text = text\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "        text = text.lower()\n",
    "        return text\n",
    "\n",
    "    df_original['text2'] = df_original['text'].apply( tweet_cleaner )\n",
    "\n",
    "    \n",
    "    # PARTE 2 - Tokeniza usando o Keras\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words,lower=True, split=\" \")\n",
    "    tokenizer.fit_on_texts(df_original['text2'])\n",
    "    df_original['text3'] = tokenizer.texts_to_sequences(df_original['text2'])\n",
    "    df_original['text4'] = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        df_original['text3'], \n",
    "        maxlen=max_len,\n",
    "        #padding='post',\n",
    "        #truncating='post'        \n",
    "    ).tolist()                                                                                                   \n",
    "       \n",
    "    # PARTE 3 - Separa amostra de treinamento e de teste\n",
    "   \n",
    "    # 1 = train | 0 = test\n",
    "    df_original['rand'] = pd.Series([0,1]).sample(len(df_original), replace=True).array\n",
    "    \n",
    "    df_train_sz = 20000\n",
    "    df_train = df_original[ df_original[ 'rand' ] == 1 ]\n",
    "    df_train_positive = df_train[ df_train['sentiment'] == 0 ].sample(n=int(df_train_sz/2), replace=True)\n",
    "    df_train_negative = df_train[ df_train['sentiment'] == 1 ].sample(n=int(df_train_sz/2), replace=True)\n",
    "    df_train = pd.concat( [ df_train_negative, df_train_positive ] )    \n",
    "    df_train = df_train.sample(frac=1.0).reset_index(drop=True)\n",
    "    \n",
    "    df_test_sz = 40000\n",
    "    df_test = df_original[ df_original[ 'rand' ] == 0 ].sample(int(df_test_sz/2), replace=True)\n",
    "    df_test = df_test.sample(frac=1.0).reset_index(drop=True)\n",
    "    \n",
    "bloco()\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6320531a",
   "metadata": {
    "id": "6320531a",
    "papermill": {
     "duration": 0.013555,
     "end_time": "2022-05-23T21:43:53.739715",
     "exception": false,
     "start_time": "2022-05-23T21:43:53.726160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Na tabela acima é possível ver os estagios da limpeza:\n",
    "* **text** contém o texto original da base \n",
    "* **text2** contém o texto após limpeza de tags, urls, nomes de usuário, números e minusculas\n",
    "* **text3** contém o texto codificado em \"embeddings\" pelo tensorflow. Cada palavra foi convertida em um número. \n",
    "* **text4** contém o texto codificado em \"embeddings\" com o padding. Só aparecem zeros aqui pois uma coluna com 1,2,3 em text3 será codificada como 0,0,0,0[...],1,2,3. Como é raro encontrar um tweet com mais de 180 palavras, o início é quase sempre [0,0,0,0,...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4064ac",
   "metadata": {
    "id": "8d4064ac",
    "papermill": {
     "duration": 0.013515,
     "end_time": "2022-05-23T21:43:53.766929",
     "exception": false,
     "start_time": "2022-05-23T21:43:53.753414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modelos - Clássicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84daced6",
   "metadata": {
    "id": "84daced6",
    "papermill": {
     "duration": 0.481475,
     "end_time": "2022-05-23T21:43:54.262043",
     "exception": false,
     "start_time": "2022-05-23T21:43:53.780568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Aplicando o Bagging com Arvores de Decisão\n",
    "bag_model = BaggingRegressor(\n",
    "    base_estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=100,\n",
    "    max_samples=0.7,\n",
    "    max_features=1.0,\n",
    "    bootstrap=True,\n",
    "    bootstrap_features=False,\n",
    "    random_state=123,\n",
    "    n_jobs=1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "lr_model = LogisticRegression(random_state=0)\n",
    "nb_model = MultinomialNB()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b4e3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "704b4e3d",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "1b2dd3b7-b2a5-4146-9917-1e03ab491467",
    "papermill": {
     "duration": 0.361638,
     "end_time": "2022-05-23T21:43:54.812477",
     "exception": false,
     "start_time": "2022-05-23T21:43:54.450839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = np.stack( df_train['text4'] )\n",
    "y_train = df_train['sentiment'].values\n",
    "\n",
    "print( f'X.shape={X_train.shape} y.shape={y_train.shape}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wf1T8_laZsYz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wf1T8_laZsYz",
    "outputId": "c50cea54-c4ce-43e7-f342-d55c8b744c87"
   },
   "outputs": [],
   "source": [
    "bag_model.fit(X_train, y_train)\n",
    "lr_model.fit(X_train, y_train)\n",
    "nb_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d101c",
   "metadata": {
    "id": "1f6d101c",
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.405961,
     "end_time": "2022-05-23T21:50:15.717400",
     "exception": false,
     "start_time": "2022-05-23T21:50:15.311439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def block(y_test, y_pred, title):\n",
    "    \n",
    "    global confusion_mtx\n",
    "    global confusion_mtx_pc\n",
    "    \n",
    "    confusion_mtx = tf.math.confusion_matrix( y_test, y_pred )\n",
    "\n",
    "    confusion_mtx = pd.DataFrame( confusion_mtx.numpy() )\n",
    "    confusion_mtx.loc['Total'] = confusion_mtx.sum(numeric_only=True) \n",
    "    confusion_mtx['total'] = confusion_mtx[0] + confusion_mtx[1]\n",
    "    \n",
    "    confusion_mtx_pc = confusion_mtx / len(y_test)\n",
    "    \n",
    "    fp = confusion_mtx.iloc[0,1] \n",
    "    fn = confusion_mtx.iloc[1,0]\n",
    "    tn = confusion_mtx.iloc[0,0]\n",
    "    tp = confusion_mtx.iloc[1,1] \n",
    "    \n",
    "    total = (tp+tn+fp+fn)\n",
    "    \n",
    "    acc       = (tp+tn)/(tp+tn+fp+fn)\n",
    "    recall    = tp/(tp+fn)\n",
    "    f1        = (2*acc*recall)/(acc+recall)\n",
    "    \n",
    "    fdr  = fp/(fp+tp)\n",
    "    fnr  = fn/(fn+tp)\n",
    "    \n",
    "    tpr = tp/(fn+tp)\n",
    "    ppv  = tp/(fp+tp)\n",
    "    \n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(f\"\"\"\n",
    "    \n",
    "        <style>\n",
    "           .luc_confusion_mtx td {{ background: white!IMPORTANT; border: 0pt !IMPORTANT; text-align: center!IMPORTANT }}           \n",
    "           td.luc_confusion_mtx_dp {{ width: 90pt; height: 90pt; background: #c0ffc0!IMPORTANT; border: 1pt solid black!IMPORTANT }} \n",
    "           td.luc_confusion_mtx_dn {{ width: 90pt; height: 90pt; background: #ffc0c0!IMPORTANT; border: 1pt solid black!IMPORTANT }}            \n",
    "        </style>\n",
    "        \n",
    "        <h4>{title}</h4>\n",
    "        <table class='luc_confusion_mtx'>\n",
    "        <tr>\n",
    "            <td></td>\n",
    "            <td></td>\n",
    "            <td colspan=2>Previsão</td>\n",
    "            <td></td>\n",
    "            <td rowspan=5 style='text-align: left!IMPORTANT'>\n",
    "                    Acurácia<br><big><big>(TP+TN)/(total)</big></big> = {tp+tn}/{total} = <big>{(tp+tn)*100.0/total:2.1f}%</big><br>\n",
    "                    <br><br>\n",
    "                    Considerando que as duas classes (0=sentimento negativo;1=sentimento positivo) tem igual \n",
    "                    valor para esta análise, é importante maximizar a diagonal verde / minimizar a diagonal vermelha,\n",
    "                    portanto os indicadores de Acurácia OU F1 são os mais indicados.<br>\n",
    "                    <br>\n",
    "                    Os indicadores de precisão e sensibilidade(recall) podem ser usados em conjunto, mas não são\n",
    "                    muito intuitivos para este conjunto de dados pois mensuram da perspectiva do \"sentimento positivo\".\n",
    "                    Em outras palavras, a sensibilidade indica quantos \"sentimentos positivos\" corretos foram encontrados\n",
    "                    e a \"precisão\" indica do total apontado pelo modelo como \"sentimento positivo\", quantos eram. \n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td></td>\n",
    "            <td></td>\n",
    "            <td>Negativo</td>\n",
    "            <td>Positivo</td>\n",
    "            <td></td>            \n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td rowspan=2>Real</td>\n",
    "            <td>Negativo</td>\n",
    "            <td class=\"luc_confusion_mtx_dp\"><big><big>TN</big></big><br>{tn}<br>{tn*100.0/total:2.1f}%</td>\n",
    "            <td class=\"luc_confusion_mtx_dn\"><big><big>FP</big></big><br>{fp}<br>{fp*100.0/total:2.1f}%</td>\n",
    "            <td>{tn+fp}</td>\n",
    "        </tr>        \n",
    "        <tr>\n",
    "            <td>Positivo</td>\n",
    "            <td class=\"luc_confusion_mtx_dn\"><big><big>FN</big></big><br>{fn}<br>{fn*100.0/total:2.1f}%</td>\n",
    "            <td class=\"luc_confusion_mtx_dp\"><big><big>TP</big></big><br>{tp}<br>{tp*100.0/total:2.1f}%</td>\n",
    "            <td>{fn+fp}</td>\n",
    "        </tr>  \n",
    "        <tr>\n",
    "            <td></td>\n",
    "            <td></td>\n",
    "            <td>{tn+fn}</td>\n",
    "            <td>{fp+tp}</td>\n",
    "            <td>{total}</td>\n",
    "        </tr>  \n",
    "        </table>    \n",
    "        \n",
    "        \n",
    "       \n",
    "    \"\"\"))        \n",
    "    \n",
    "    print( f'             Acurácia={acc*100.0:05.2f}% dos apontamentos positivos e negativos estão corretos' )\n",
    "    print( f' Recall/Sensibilidade={tpr*100.0:05.2f}% dos sentimentos positivos da base foram apontados' )   \n",
    "    print( f'             Precisão={ppv*100.0:05.2f}% dos sentimentos positivos apontados estão corretos' )    \n",
    "    print( f'             F1 Score={f1*100.0:05.2f}% média harmônica da acurácia e recall' )        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f8042",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9f1f8042",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "73bb09b5-fa0e-40ad-a2e7-54a36e4fee96",
    "papermill": {
     "duration": 24.377452,
     "end_time": "2022-05-23T21:50:14.947685",
     "exception": false,
     "start_time": "2022-05-23T21:49:50.570233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = np.stack( df_test['text4'] )\n",
    "y_test = df_test['sentiment'].values\n",
    "y_pred_bag = [ 1 if y_pred > 0.5 else 0 for y_pred in bag_model.predict(X_test).reshape(len(X_test)) ]\n",
    "y_pred_lr = [ 1 if y_pred > 0.5 else 0 for y_pred in lr_model.predict(X_test).reshape(len(X_test)) ]\n",
    "y_pred_nb = [ 1 if y_pred > 0.5 else 0 for y_pred in nb_model.predict(X_test).reshape(len(X_test)) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff013e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "89ff013e",
    "outputId": "dc71454a-91e3-4378-ca7d-9de389170cc5",
    "papermill": {
     "duration": 0.36414,
     "end_time": "2022-05-23T21:50:16.467544",
     "exception": false,
     "start_time": "2022-05-23T21:50:16.103404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "block(y_test, y_pred_bag, 'Método Bagging com Árvore de Decisão')\n",
    "block(y_test, y_pred_lr, 'Método Regressão Logística')\n",
    "block(y_test, y_pred_nb, 'Método MultinomialNB')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "an-lise-de-sentimento_Metodos_Classicos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_tf",
   "language": "python",
   "name": "venv_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 487.659147,
   "end_time": "2022-05-23T21:50:19.655589",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-23T21:42:11.996442",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
